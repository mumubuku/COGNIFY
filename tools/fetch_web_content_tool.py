# tools/fetch_web_content_tool.py
from registry.tool_registry import register_tool
import httpx
from readability import Document
from selectolax.parser import HTMLParser

@register_tool
class FetchWebContentTool:
    @staticmethod
    def get_tool_schema():
        return {
            "name": "FetchWebContentTool",
            "description": "æŠ“å–ç½‘é¡µæ­£æ–‡å†…å®¹ï¼ˆè‡ªåŠ¨æå–ï¼Œå…¼å®¹å¤§éƒ¨åˆ†æ–°é—»ã€åšå®¢ç½‘ç«™ï¼‰",
            "parameters": {
                "type": "object",
                "properties": {
                    "url": {
                        "type": "string",
                        "description": "éœ€è¦æŠ“å–æ­£æ–‡å†…å®¹çš„ç½‘é¡µURL"
                    }
                },
                "required": ["url"]
            }
        }

    async def run(self, arguments: dict) -> str:
        url = arguments["url"]
        print(f"ğŸŒ æŠ“å–ç½‘é¡µå†…å®¹: {url}")

        try:
            async with httpx.AsyncClient(timeout=30.0, follow_redirects=True) as client:
                response = await client.get(url)
                response.raise_for_status()

                html = response.text

                # ä½¿ç”¨ readability ä¼˜å…ˆæå–æ­£æ–‡
                doc = Document(html)
                content_html = doc.summary()
                parser = HTMLParser(content_html)

                # è¿›ä¸€æ­¥æå–å¹²å‡€çš„æ–‡å­—æ®µè½
                paragraphs = []
                for p_tag in parser.css('p'):
                    text = p_tag.text(strip=True)
                    if len(text) > 30:
                        paragraphs.append(text)

                content = "\n\n".join(paragraphs)
                if not content:
                    return "æœªèƒ½æå–æœ‰æ•ˆæ­£æ–‡ï¼Œè¯·æ£€æŸ¥ç½‘é¡µæ˜¯å¦ä¸ºåŠ¨æ€æ¸²æŸ“ã€‚"
                
                return content[:5000]  # é™åˆ¶æœ€å¤§è¿”å›é•¿åº¦
        except Exception as e:
            print(f"âŒ æŠ“å–å¤±è´¥: {e}")
            return f"æŠ“å–å¤±è´¥: {e}"

# workflows/search_summarize_workflow.py
from workflows.workflow_base import WorkflowBase
from schemas.task_input import TaskInput
from schemas.task_result import TaskResult
from model_client import ask_deepseek
from function_manager import FunctionManager

class SearchSummarizeWorkflow(WorkflowBase):
    async def run(self, task: TaskInput) -> TaskResult:
        print(f"ğŸš€ å¼€å§‹æœç´¢æ€»ç»“ä»»åŠ¡: {task.task}")

        # Step 1: æœç´¢é˜¶æ®µ
        search_result = await FunctionManager.execute_function("SearchWebTool", {"query": task.task})
        print("\nğŸ” [æœç´¢ç»“æœå†…å®¹] å‰500å­—ï¼š\n", search_result[:500])

        links = self.extract_links(search_result)
        print(f"\nğŸ”— [æå–åˆ°çš„æœ‰æ•ˆé“¾æ¥æ•°é‡]: {len(links)}")
        for idx, link in enumerate(links, 1):
            print(f"  {idx}. {link}")

        if not links:
            return TaskResult(
                content="æœªèƒ½æ‰¾åˆ°ç›¸å…³é“¾æ¥ã€‚",
                used_tools=["SearchWebTool"],
                logs_path=""
            )

        all_summaries = []
        used_tools = ["SearchWebTool", "FetchWebContentTool"]

        # Step 2: æŠ“å–å’Œå±€éƒ¨æ€»ç»“é˜¶æ®µ
        for idx, link in enumerate(links, 1):
            print(f"\nğŸŒ å¼€å§‹æŠ“å–ç¬¬ {idx} ä¸ªé“¾æ¥å†…å®¹: {link}")

            # æŠ“å–æ­£æ–‡
            content = await FunctionManager.execute_function("FetchWebContentTool", {"url": link})
            if not content or "å¤±è´¥" in content:
                print(f"âš ï¸ æŠ“å–å¤±è´¥æˆ–å†…å®¹æ— æ•ˆï¼Œè·³è¿‡è¯¥é“¾æ¥ã€‚")
                continue
            print(f"âœ… æŠ“å–æˆåŠŸï¼Œæ­£æ–‡å‰300å­—ï¼š\n{content[:300]}")

            # æ€»ç»“æ­£æ–‡
            print("âœï¸ å¼€å§‹æ€»ç»“ç½‘é¡µå†…å®¹...")
            summary_result = await ask_deepseek(
                f"è¯·æ€»ç»“ä»¥ä¸‹ç½‘é¡µå†…å®¹ï¼Œæ§åˆ¶åœ¨300å­—ä»¥å†…ï¼š\n{content[:4000]}",
                use_function_calling=False
            )
            print(f"âœ… å•é¡µæ€»ç»“å®Œæˆï¼Œå‰300å­—ï¼š\n{summary_result.content[:300]}")

            all_summaries.append(f"ğŸ”— {link}\n{summary_result.content}\n")

        # Step 3: æ•´åˆç»¼åˆæ€»ç»“é˜¶æ®µ
        if all_summaries:
            print("\nğŸ›  å¼€å§‹ç»¼åˆå¤šç½‘é¡µæ€»ç»“...")
            combined_text = "\n\n".join(all_summaries)
            final_summary = await ask_deepseek(
                f"ä»¥ä¸‹æ˜¯å¤šä¸ªç½‘é¡µå†…å®¹çš„æ€»ç»“ï¼Œè¯·é‡æ–°å½’çº³æˆä¸€ä»½é€»è¾‘æ¸…æ™°ã€å®Œæ•´å‡†ç¡®ã€æ§åˆ¶åœ¨800å­—ä»¥å†…çš„å¤§æ€»ç»“ï¼š\n{combined_text}",
                use_function_calling=False
            )
            final_report = final_summary.content
            print("\nğŸ“„ [æœ€ç»ˆç»¼åˆæ€»ç»“å‰500å­—]ï¼š\n", final_report[:500])
        else:
            final_report = "æœªèƒ½æœ‰æ•ˆæ€»ç»“ä»»ä½•ç½‘é¡µã€‚"
            print("\nâš ï¸ æœ€ç»ˆæ— æœ‰æ•ˆå†…å®¹å¯æ€»ç»“ã€‚")

        return TaskResult(
            content=final_report,
            used_tools=used_tools,
            logs_path=""
        )

    @staticmethod
    def extract_links(search_text: str) -> list:
        import re
        pattern = r"ğŸ”— (https?://[^\s]+)"
        return re.findall(pattern, search_text)
